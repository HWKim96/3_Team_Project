{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">**GridSearchCV**</span>를 통해 **XGBoost**의 최적의 파라미터 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "# 가능한 모든 파라미터 조합에 대해 교차 검증을 수행하여 최적의 파라미터를 찾는 라이브러리\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('../future_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y train은 기존 train데이터로 date_block_num 34 이전까지 사용\n",
    "X_train = train[train['date_block_num'] < 34].drop(['item_cnt_month'], axis=1)\n",
    "y_train = train[train['date_block_num'] < 34]['item_cnt_month']\n",
    "\n",
    "model = XGBRegressor()\n",
    "\n",
    "# GridSearch에서 체크할 파라미터를 입력\n",
    "param_grid = {\n",
    "    'max_depth': [9],\n",
    "    'min_child_weight': [200, 300, 400],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'eta': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=4)\n",
    "                    # model = XGBoost                   # cv(cross_validation): 데이터를 3개의 fold로 나눠\n",
    "                                                        # 2개는 train, 1개는 test로 3번 반복해 모델을 학습하고 평가하여 성능 계산\n",
    "                                                        # n_jobs: 병렬처리에 사용될 작업 수 지정(CPU 코어 사용할 개수)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Best score: \", grid.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost의 주요 Parameter\n",
    "- max_depth : 트리의 최대 깊이 \n",
    "\n",
    "        [default: 6]\n",
    "\n",
    "- learning_rate : 각 트리 가중치에 곱해지는 값, 학습 단계별 이전 결과를 얼마나 반영할지 설정 \n",
    "\n",
    "        [default: 0.3]\n",
    "\n",
    "- n_estimators : 학습할 트리의 개수 \n",
    "\n",
    "        [default: 100]\n",
    "\n",
    "- min_child_weight : child에서 필요한 모든 관측치에 대한 가중치의 최소 합계, 높을수록 모델이 덜 복잡해짐, 너무 크면 과소적합 가능성 있음 \n",
    "\n",
    "        [default: 1]\n",
    "\n",
    "- colsample_bytree : 각 트리마다 사용되는 feature 샘플링 비율 \n",
    "\n",
    "        [default: 1]\n",
    "\n",
    "- subsample : 각 트리마다 사용되는 샘플링 비율 \n",
    "\n",
    "        [default: 1]\n",
    "\n",
    "- gamma : 트리에서 추가 가지를 나눌지 말지 결정하는 최소 손실 감소 값, 높을수록 보수적으로 학습, 클수록 과적합이 감소하는 효과가 있음 \n",
    "\n",
    "        [default: 0]\n",
    "\n",
    "- reg_alpha : L1규제 (Lasso), 높이면 모델이 단순화 \n",
    "\n",
    "        [default: 0]\n",
    "\n",
    "- reg_lambda : L2규제 (Ridge), 높이면 모델이 단순화 \n",
    "\n",
    "        [default: 1]\n",
    "\n",
    "- eta : 각 트리가 가지는 가중치의 크기를 제어, 작으면 정교하게 학습하지만 시간이 길어짐 \n",
    "\n",
    "        [default: 0.3]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에 적히지 않는 파라미터는 전부 기본값을 사용했습니다.\n",
    "\n",
    "---\n",
    "param_grid_set_1\n",
    "```\n",
    "    {'max_depth': 10, 'min_child_weight': [200, 300, 400], 'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'subsample': [0.6, 0.7, 0.8], 'eta': [0.1, 0.2, 0.3]}\n",
    "\n",
    "# Best parameters: {'colsample_bytree': 0.6, 'eta': 0.3, 'max_depth': 10, 'min_child_weight': 200, 'subsample': 0.8}\n",
    "# Best score:  0.5341463698480631\n",
    "```\n",
    "---\n",
    "param_grid_set_2\n",
    "```\n",
    "    {'max_depth': 9, 'min_child_weight': [200, 300, 400], 'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'subsample': [0.6, 0.7, 0.8], 'eta': [0.1, 0.2, 0.3]}\n",
    "\n",
    "#  Best parameters:  {'colsample_bytree': 0.7, 'eta': 0.3, 'max_depth': 9, 'min_child_weight': 200, 'subsample': 0.8}\n",
    "#  Best score:  0.5307821579919747\n",
    "```\n",
    "---\n",
    "param_grid_set_3\n",
    "```\n",
    "\n",
    "    {'max_depth': 8, 'min_child_weight': [200, 300, 400], 'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'subsample': [0.6, 0.7, 0.8], 'eta': [0.1, 0.2, 0.3]}\n",
    "\n",
    "# Best parameters:  {'colsample_bytree': 0.6, 'eta': 0.1, 'max_depth': 8, 'min_child_weight': 400, 'subsample': 0.6}\n",
    "# Best score:  0.46384771589926627\n",
    "```\n",
    "---\n",
    "param_grid_set_4\n",
    "```\n",
    "    {'max_depth': 7, 'min_child_weight': [200, 300, 400], 'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'subsample': [0.6, 0.7, 0.8], 'eta': [0.1, 0.2, 0.3]}\n",
    "\n",
    "# Best parameters:  {'colsample_bytree': 0.6, 'eta': 0.1, 'max_depth': 7, 'min_child_weight': 300, 'subsample': 0.8}\n",
    "# Best score:  0.46341746320238447\n",
    "```\n",
    "---\n",
    "param_grid_set_5\n",
    "```\n",
    "    {'max_depth': [5, 6], 'min_child_weight': [200, 300, 400, 500], 'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9], 'eta': [0.1, 0.2, 0.3, 0.4]}\n",
    "\n",
    "# Best parameters:  {'colsample_bytree': 0.9, 'eta': 0.1, 'max_depth': 6, 'min_child_weight': 300, 'subsample': 0.7}\n",
    "# Best score:  0.4640190891895628\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ab5f114bd244db9246c69e02fd439101cc738e2a0d68f69c6c889940936d20b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
